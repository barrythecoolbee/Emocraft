behaviors:
  SimpleCollector:
    trainer_type: ppo
    hyperparameters:
      batch_size: 128 #os parâmetros da rede neuronal são atualizados a cada 128
      buffer_size: 2048 #partido em 16 batches
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256 
      num_layers: 2 #rede com 2 camadas 
      vis_encode_type: simple
    reward_signals:
      extrinsic: #são as recompensas do ambiente
        gamma: 0.99 #pesar recompensas futuras (fator de desconto)
        strength: 1.0
    keep_checkpoints: 5
    max_steps: 20000000
    time_horizon: 128 #ao fim de 128 steps faz-se uma estimativa da recompensa total do episódio o que é útil para quando temos recompensas esparsas (demasiadas vezes 0) ou episódios muito grandes
    summary_freq: 20000 #envia para a consola a cada 20 000
    threaded: true